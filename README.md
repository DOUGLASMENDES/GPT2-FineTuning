# GPT-2 Small Fine-Tuning

## ğŸ“„ DescriÃ§Ã£o
Este projeto demonstra o processo de fine-tuning do modelo GPT-2 small utilizando a biblioteca Transformers da Hugging Face e Google Colab (de forma gratuita com GPU T4).

## ğŸš€ Objetivo
Realizar o fine-tuning do GPT-2 (small - 124 milhÃµes de parÃ¢metros) em um dataset de diÃ¡logos (DailyDialog) para aprimorar a geraÃ§Ã£o de texto em contexto de conversaÃ§Ã£o.

## ğŸ›  Tecnologias Utilizadas
- Python
- PyTorch
- Transformers (Hugging Face)
- Datasets
- Accelerate
- PEFT
- Bitsandbytes

## ğŸ“‚ Estrutura do Projeto
```
ğŸ“¦ GPT2_Fine_Tuning_Portfolio
â”œâ”€ ğŸ“‚ data/                # Dados utilizados no treinamento
â”œâ”€ ğŸ“‚ logs/                # Logs do treino
â”œâ”€ ğŸ“‚ images/              # GrÃ¡ficos de desempenho e uso de recursos
â”œâ”€ ğŸ“‚ gpt2_finetuned/      # Modelo treinado e checkpoints
â””â”€ GPT2_fine_tuning.ipynb  # Notebook do projeto
```

## ğŸ” MÃ©tricas de Treinamento
Durante o treinamento, foram monitoradas as seguintes mÃ©tricas:
- **Loss:** A perda diminuiu de forma consistente, atingindo ~0.2676 no final.
- **Learning Rate:** A taxa de aprendizado decaiu linearmente para estabilizar o treinamento.
- **Grad Norm:** O gradiente apresentou oscilaÃ§Ãµes, mas sem comprometer a perda.

### ğŸ“ˆ GrÃ¡ficos
Os grÃ¡ficos abaixo mostram o comportamento do treinamento e o uso de recursos do sistema:
- **Treinamento:** Perda, taxa de aprendizado, gradiente, Ã©pocas e passos globais.
- **Sistema:** Uso de CPU, GPU, memÃ³ria, trÃ¡fego de rede e disco.


![Performance do Treino](./images/chart_train.png)
![GPU](./images/chart_gpu.png)
![GPU 2](./images/chart_gpu2.png)
![MemÃ³ria](./images/chart_memory.png)
![CPU/Disco](./images/chart_cpu_disk.png)


## ğŸ’» ExecuÃ§Ã£o do Projeto
1. Clone o repositÃ³rio:
```bash
git clone https://github.com/douglasmendes/GPT2-FineTuning.git
cd GPT2-FineTuning
```

2. Instale as dependÃªncias:
```bash
pip install -r requirements.txt
```

3. Execute o notebook:
```bash
jupyter notebook GPT2_fine_tuning.ipynb
```

## ğŸ“Š Resultados
O modelo fine-tunado foi capaz de gerar respostas contextuais em diÃ¡logos, mostrando uma boa adaptaÃ§Ã£o ao dataset DailyDialog. A perda final foi de aproximadamente 0.2676, o que indica um bom aprendizado do modelo.

## ğŸ¤– Exemplo de Uso
```python
from transformers import AutoModelForCausalLM, AutoTokenizer
model = AutoModelForCausalLM.from_pretrained("./gpt2_finetuned")
tokenizer = AutoTokenizer.from_pretrained("gpt2")

input_text = "OlÃ¡, como vocÃª estÃ¡?"
inputs = tokenizer(input_text, return_tensors="pt")
output = model.generate(**inputs, max_length=50)
print(tokenizer.decode(output[0], skip_special_tokens=True))
```

## ğŸ¯ PrÃ³ximos Passos
- Avaliar o modelo em cenÃ¡rios reais de conversaÃ§Ã£o.
- Explorar outras tÃ©cnicas de fine-tuning e datasets maiores.
- Implementar melhorias no pipeline de dados para maior eficiÃªncia.

## ğŸ“ LicenÃ§a
Este projeto Ã© licenciado sob a MIT License - veja o arquivo [LICENSE](LICENSE) para mais detalhes.

---
